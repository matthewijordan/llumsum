Here’s a high-level design and implementation guide for the `llumsum` tool.

---

# 🧠 `llumsum` – LLM-Powered Codebase Summarizer

## 📌 Summary

`llumsum` is a lightweight, language-agnostic summarization tool for source code projects, designed to assist large language model agents and developer tools by maintaining structured, high-level summaries of code files. It mirrors a project’s directory tree in a hidden `.llumsum/` folder, storing plain-text summaries and metadata (like content hashes) to track and manage source understanding at scale.

It is intended as a reusable library and CLI companion module, with modular encapsulation and clean API surface, and integrates with LangChain to allow flexible LLM backend selection.

---

## 🎯 Motivation

AI coding assistants and agents suffer from context limitations and expensive re-ingestion of large or irrelevant files. Human developers solve this with mental models: knowing which files matter and what they do at a glance.

`llumsum` formalizes that intuition: a tool- and language-neutral system that summarizes what a file does, what it exports, and what workflows it’s part of—storing this alongside the project for fast retrieval and incremental regeneration.

---

## 🛠️ Intended Use Cases

* **CLI-based LLM assistants** to load summaries of relevant files instead of raw code
* **Multi-agent LLM systems** to determine file relevance, search paths, or perform scoped planning
* **Developer productivity tools** to provide lightweight, browsable overviews of large codebases
* **Offline or background summarization** using file watchers or git hooks

---

## 🧱 High-Level Architecture

```
my-project/
├── src/
│   └── main.ts
└── .llumsum/
    └── src/
        └── main.ts.summary
```

Each `.summary` file contains:

* A hash of the file content (used for staleness detection)
* A free-form natural language summary (generated by an LLM)

---

## 📦 Module Structure (Proposed)

```
packages/llumsum/
├── src/
│   ├── index.ts                  // Public API
│   ├── summarizer/               // LangChain + LLM interfaces
│   ├── fs/
│   │   ├── layout.ts             // Summary path resolution logic
│   │   ├── readWrite.ts          // File system helpers
│   │   └── hash.ts               // Hashing utilities
│   ├── cli/
│   │   └── index.ts              // Optional CLI integration
│   └── types/
│       └── summary.ts            // Type definitions
├── .llumsum/project.config.json // Project-specific configuration
├── .llumsum/personal.config.json // Personal configuration (ignored by Git)
└── package.json
```

---

## 🧩 Key Components

### 1. **File Mirroring Layer**

* Resolves `.llumsum/` mirror path for a given source file
* Ensures directory structure is created on demand
* Mirrors source file locations exactly, appending `.summary` as extension

### 2. **Summary Format**

* First line: `hash: <sha1>` (or other hash)
* Body: LLM-generated free-form natural language summary

  * High-level file purpose
  * Exported symbols & their roles
  * Important workflows or side effects

### 3. **LLM Summarizer (LangChain-based)**

* Abstracted LangChain `Runnable` chain for prompt + model interaction
* LLM configuration is determined by the following hierarchy (first one found is used):
    1.  `llmApiKey` in `.llumsum/personal.config.json`
    2.  `llmApiKey` in `.llumsum/project.config.json`
    3.  `LLMSUM_OPENAI_API_KEY` environment variable

* Other LLM parameters (`llmModel`, `llmApiBase`) are determined by:
    1.  `.llumsum/project.config.json`
    2.  `LLMSUM_LLM_MODEL` / `LLMSUM_OPENAI_API_BASE` environment variables

### 4. **Hashing & Sync Logic**

* Efficient content hash comparison (e.g. SHA-1)
* Dirty detection: file hash mismatch triggers regeneration
* Optional file watcher or CLI `sync` command to batch update summaries

### 5. **Public API Surface**

```ts
// Read a summary (regenerates if needed)
readOrGenerateSummary(filePath: string): Promise<SummaryResult>

// Force regeneration
generateSummary(filePath: string): Promise<SummaryResult>

// Read summary (no LLM fallback)
readSummaryFile(filePath: string): Promise<SummaryResult | null>

// Utility: check if summary is stale
isStale(filePath: string): Promise<boolean>
```

---

## 🖥️ CLI Interface

```bash
llumsum sync [--force]     # Regenerates stale summaries, removes orphan ones, and tracks token usage. Use --force to regenerate all.
llumsum save-summary path.ts # Reads or generates a single summary and saves it.
llumsum get-summary path.ts  # Gets a summary for a single file. Generates if needed. Does not save if outside project.
```

---

## ⚙️ Configuration

Project-specific configuration is managed in `.llumsum/project.config.json`. If this file does not exist, it will be created with default values.

```json
{
  "syncConfirmationThreshold": 50,
  "ignorePatterns": [
    "node_modules/**",
    ".git/**",
    ".llumsum/**",
    "dist/**",
    "temp/**",
    "package-lock.json"
  ],
  "llmModel": "google/gemini-2.5-flash",
  "llmApiBase": "https://openrouter.ai/api/v1"
}
```

*   `syncConfirmationThreshold`: (number) The maximum number of files that can be summarized during a `sync` operation before a user confirmation prompt is displayed. Defaults to `50`.
*   `ignorePatterns`: (array of strings) A list of glob patterns for files and directories to ignore during `sync` operations. Summaries will not be generated for these files, and they will not trigger confirmation prompts. Defaults to common development artifacts.
*   `llmModel`: (string) The specific LLM model to use for summarization (e.g., `gpt-4-turbo`, `anthropic/claude-3-haiku`).
*   `llmApiBase`: (string) The base URL for the LLM API (e.g., `https://api.openai.com/v1`, `https://openrouter.ai/api/v1`).
*   `parallelism`: (number) The maximum number of concurrent LLM calls to make during a `sync` operation. Defaults to `8`.

Personal configuration, primarily for sensitive API keys, is managed in `.llumsum/personal.config.json`. This file should **not** be committed to version control.

```json
{
  "llmApiKey": "your_llm_api_key"
}
```

*   `llmApiKey`: (string) Your API key for the OpenAI-compatible LLM provider.

---

## 🔐 Design Principles

*   **Encapsulation**: Filesystem logic, LangChain config, and summary format are separated.
*   **Stateless Core**: Summary logic is deterministic and easy to run in CI, watch loops, or remote agents.
*   **Composable**: Designed for direct use in CLI tools, LLM orchestrators, or background services.
*   **Extensible**: Support for multiple summary styles (e.g. docstring-only, full workflow) in future.
*   **Provider-Agnostic**: Supports any LangChain-compatible model; selection is fully externalized.

---

## 🌱 Future Considerations

*   Summary caching/indexing (e.g. `.llumsum/manifest.json`)
*   Git integration: generate summaries on diffs or pre-commit
*   Language-aware extensions (e.g. function signature detection from AST)
*   Custom prompt styles per project/language

---
